{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter10_natural-language-processing/10.7_sentiment-analysis-rnn\n",
    "IMDB数据集：http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "'''\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 18907.19it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 20979.81it/s]\n",
      "100%|██████████| 12500/12500 [00:04<00:00, 2612.48it/s]\n",
      "100%|██████████| 12500/12500 [00:04<00:00, 2653.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white chicks hold on, why couldn\\'t they have dressed as black chicks, oh yeah, they wouldn\\'t look different at all. can anyone give me one wayans movie where they haven\\'t dressed up as ladies? don\\'t be a menace doesn\\'t count, jack white and michael costanza ghost wrote that (the other norton trio members acted as directors).<br /><br />in white chicks, there\\'s never really any jokes. it\\'s just the wayans acting like girls for 2 hours. there\\'s no setups, no punchlines and no laughs. there is a lot of \"i think i\\'m gonna play some time crisis 3.\" at least for me there was (5 times to be exact).<br /><br />somebody has to tell kenan ivory, damon, marlon, shawn, damien (the only talented one), kim, rakeesha, george w., and osama bin wayans to stop making movies. its only hurting the o-zone layer.<br /><br />verdict 1/2* out of ****', 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#加载数据集  文件->句子的list i.e ['i think you xx xx xx nice xx', 1]\n",
    "def read_file(folder='train', data_root=\"/Users/jiang/OneDrive/学习/NLP/practice/3/aclImdb\"):\n",
    "    data = []\n",
    "    for label in ['neg', 'pos']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "  \n",
    "train_data, test_data = read_file('train'), read_file('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子划分成单个词、换成小写\n",
    "def get_tokenized(data):\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(text) for text, _ in data]\n",
    "\n",
    "#创建词典\n",
    "def get_word2index(data):\n",
    "    tokenizer_data = get_tokenized(data)\n",
    "    counter = collections.Counter([tok for seq in tokenizer_data for tok in seq])\n",
    "    return Vocab.Vocab(counter, min_freq=5)  #过滤掉出现次数少于5的词\n",
    "\n",
    "word2index = get_word2index(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将单词转化成小写，通过截断或者补0来将每条评论长度固定成500\n",
    "def preprocess(data, word2index):\n",
    "    max_l = 500  #每条评论长度不一， 将每条评论通过截断或者补0，使得长度变成500\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>=max_l else x + [0] * (max_l-len(x))\n",
    "    \n",
    "    tokenizer_data = get_tokenized(data)\n",
    "    features = torch.tensor([pad([word2index[word] for word in words]) for words in tokenizer_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels   #features:句子中的单词index的列表， labels：目标值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 391)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "#Data.TensorDataset:包装数据和目标张量的数据集\n",
    "#Data.DataLoader\n",
    "train_set = Data.TensorDataset(*preprocess(train_data, word2index))\n",
    "test_set = Data.TensorDataset(*preprocess(test_data, word2index))\n",
    "train_iter = Data.DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, BATCH_SIZE)\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLSTM输入: input, (h_0, c_0)\\ninput (seq_len, batch, input_size)\\nh_0 (num_layers * num_directions, batch, hidden_size):保存着batch中每个元素的初始化隐状态的Tensor\\nc_0 (num_layers * num_directions, batch, hidden_size): 保存着batch中每个元素的初始化细胞状态的Tensor\\n\\nLSTM输出 output, (h_n, c_n)\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        #num_layers:rnn层数  bidirectional：双向rnn\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size=num_hiddens, num_layers=num_layers, bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))  #inputs：[BATCH_SIZE, seq_length] embeddings;[seq_length, BATCH_SIZE, embed_size]\n",
    "        outputs, _ = self.rnn(embeddings)\n",
    "        outputs = torch.cat((outputs[0], outputs[-1]), dim=-1)\n",
    "        out = self.decoder(outputs)\n",
    "        return out\n",
    "'''\n",
    "LSTM输入: input, (h_0, c_0)\n",
    "input (seq_len, batch, input_size)\n",
    "h_0 (num_layers * num_directions, batch, hidden_size):保存着batch中每个元素的初始化隐状态的Tensor\n",
    "c_0 (num_layers * num_directions, batch, hidden_size): 保存着batch中每个元素的初始化细胞状态的Tensor\n",
    "\n",
    "LSTM输出 output, (h_n, c_n)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 cost = 0.824846\n",
      "Epoch: 1 cost = 0.720414\n",
      "Epoch: 2 cost = 0.697621\n",
      "Epoch: 3 cost = 0.699350\n",
      "Epoch: 4 cost = 0.691135\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "vocab_size = len(word2index)\n",
    "EPOCH =5\n",
    "\n",
    "model = BiLSTM(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch[0])\n",
    "        \n",
    "        loss = criterion(preds, batch[1])\n",
    "        losses.append(loss.detach().numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch:', '%01d' % epoch, 'cost =', '{:.6f}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'\n",
    "\n",
    "predict_sentiment(model, word2index, ['this', 'movie', 'is', 'so', 'great']) # positive\n",
    "predict_sentiment(model, word2index, ['this', 'movie', 'is', 'so', 'bad']) # negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
